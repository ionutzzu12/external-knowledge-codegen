
training set / dev set ??

The calls' func names are actually attributes (exp = Name(module_name), id = func_name).

augment_bin_examples -> adds functions member (index in func_details.json, func name with its snippet form).


? add doc words in vocab
? how to encode docs (encoding dimensions)


Results (apply train aka finetuning step, then test):
{'corpus_bleu': 0.2651451515021723, 'oracle_corpus_bleu': 0.3696938598275679, 'avg_sent_bleu': 0.21372752274218482, 'oracle_avg_sent_bleu': 0.33316551591050664, 'exact_match': 0.02, 'oracle_exact_match': 0.044}


- total "Call" rule nodes: 4581
- total functions extracted: 3246
- from total, not found in docs: 1560 (564 unique)

- most popular rule sequences succeeding "Call":
      46 ['Attribute', 'Subscript', 'Name']
      52 ['Attribute', 'Call', 'Attribute', 'Attribute', 'Name']
      65 ['Attribute', 'Call', 'Name']
     163 ['Attribute', 'Call', 'Attribute', 'Name']
     246 ['Attribute', 'Str']
     274 ['Attribute', 'Attribute', 'Name']
    1730 ['Name']
    1874 ['Attribute', 'Name']


GTX960m times:
--cuda
	92s  training epoch
	4:44 decoding 100 examples
	9:31 decoding 200 examples (first epoch - lasted longer)

GTX960m times after docs+pointer_net2:
--cuda
	250s  training epoch
	8:23 decoding 200 examples

GTX960m times after docs+pointer_net2+shuffle3funcs:
--dynamic cuda
	185s  training epoch
	2:53 decoding 200 examples
	

google colab times:
--cuda
	20s  training epoch
	1:20 decoding 200 examples
no_cuda
	120s  training epoch
	1:20 decoding 200 examples
	
google colab times after docs+pointer_net2:
--cuda
	211s  training epoch
	3:16 decoding 200 examples
