
The calls' func names are actually attributes (exp = Name(module_name), id = func_name).

augment_bin_examples -> adds functions member (index in func_details.json, func name with its snippet form).


? add doc words in vocab
? how to encode docs (encoding dimensions)


Results (apply train aka finetuning step, then test):
{'corpus_bleu': 0.2651451515021723, 'oracle_corpus_bleu': 0.3696938598275679, 'avg_sent_bleu': 0.21372752274218482, 'oracle_avg_sent_bleu': 0.33316551591050664, 'exact_match': 0.02, 'oracle_exact_match': 0.044}

Results (5 funcs train shuffle):
{'corpus_bleu': 0.29433628241503246, 'oracle_corpus_bleu': 0.37361012011024897, 'avg_sent_bleu': 0.23643295943373965, 'oracle_avg_sent_bleu': 0.36451956295716753, 'exact_match': 0.028, 'oracle_exact_match': 0.1}


ok, deci da la fel ca originalul si cu functiile de un token. am preprocesat numai conala-train, de aceea si vocab e mult mai mic. 

times:
--rtx2060s																	--colab
	CLASSIC									FUNCS								FUNCS
	(cuda) 40s training epoch				(cuda) 60s training epoch 			(cuda) 80s
	(cuda) 90s decoding 100					(cuda)  							(cuda)
	(cpu)  21s decoding 100					(cpu)  27s decoding 100				(cpu) 32s





- total "Call" rule nodes: 4581
- total functions extracted: 3246
- from total, not found in docs: 1560 (564 unique)

- most popular rule sequences succeeding "Call":
      46 ['Attribute', 'Subscript', 'Name']
      52 ['Attribute', 'Call', 'Attribute', 'Attribute', 'Name']
      65 ['Attribute', 'Call', 'Name']
     163 ['Attribute', 'Call', 'Attribute', 'Name']
     246 ['Attribute', 'Str']
     274 ['Attribute', 'Attribute', 'Name']
    1730 ['Name']
    1874 ['Attribute', 'Name']


GTX960m times:
--cuda
	92s  training epoch
	4:44 decoding 100 examples
	9:31 decoding 200 examples (first epoch - lasted longer)

GTX960m times after docs+pointer_net2:
--cuda
	250s  training epoch
	8:23 decoding 200 examples

google colab times:
--cuda
	20s  training epoch
	1:20 decoding 200 examples
no_cuda
	20s  training epoch
	1:20 decoding 200 examples
